{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# æ­¥é©Ÿ 1: è¼‰å…¥æ¨¡å‹\n",
    "# ----------------------------------------------------\n",
    "MODEL_PATH = \"/home/nculcwu/DeepSeek/deepseek-llm-7b-chat\"\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(f\"æº–å‚™å¾ '{MODEL_PATH}' è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨...\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"éŒ¯èª¤ï¼šæœªåµæ¸¬åˆ° CUDAã€‚æ­¤ç¨‹å¼ç¢¼éœ€è¦ NVIDIA GPU æ‰èƒ½åŸ·è¡Œã€‚\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"âœ… æ¨¡å‹å’Œåˆ†è©å™¨å·²æˆåŠŸè¼‰å…¥è‡³ GPUï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼éŒ¯èª¤è¨Šæ¯: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# æ­¥é©Ÿ 2: å®šç¾©ç¿»è­¯å‡½å¼\n",
    "# ----------------------------------------------------\n",
    "def translate_to_plain_chinese(text: str) -> str:\n",
    "    \"\"\"å‘¼å«æ¨¡å‹é€²è¡Œç¹é«”ç™½è©±æ–‡ç¿»è­¯\"\"\"\n",
    "    prompt = f\"å°‡ä»¥ä¸‹æ–‡è¨€æ–‡ç¿»è­¯æˆç¹é«”ä¸­æ–‡ç™½è©±æ–‡ï¼Œåªç¿»è­¯å…§å®¹ï¼Œä¸è¦åšè§£é‡‹æˆ–åˆ†é»ï¼š\\n\\n{text}\\n\\nç¿»è­¯ï¼š\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # å¾ç”Ÿæˆçµæœä¸­æŠ½å–ã€Œç¿»è­¯ã€éƒ¨åˆ†\n",
    "    if \"ç¿»è­¯ï¼š\" in result:\n",
    "        result = result.split(\"ç¿»è­¯ï¼š\", 1)[-1].strip()\n",
    "    return result\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# æ­¥é©Ÿ 3: æ‰¾åˆ°è³‡æ–™å¤¾è£¡çš„æ‰€æœ‰ txt æª”ï¼Œé€æª”ç¿»è­¯\n",
    "# ----------------------------------------------------\n",
    "input_dir = r\".\"\n",
    "output_file = os.path.join(input_dir, \"ç¿»è­¯çµæœ.txt\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".txt\") and file_name != \"ç¿»è­¯çµæœ.txt\":\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        print(f\"ğŸ“– æ­£åœ¨è™•ç†ï¼š{file_name}\")\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()\n",
    "        \n",
    "        if content:\n",
    "            translated = translate_to_plain_chinese(content)\n",
    "            all_results.append(f\"=== {file_name.replace('.txt','')} ===\\n{translated}\\n\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# æ­¥é©Ÿ 4: è¼¸å‡ºåˆ°å–®ä¸€æª”æ¡ˆ\n",
    "# ----------------------------------------------------\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(all_results))\n",
    "\n",
    "print(f\"\\nâœ… æ‰€æœ‰ç¿»è­¯å·²å®Œæˆï¼Œçµæœè¼¸å‡ºè‡³ï¼š{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# æ­¥é©Ÿ 1: è¼‰å…¥æ¨¡å‹\n",
    "# ----------------------------------------------------\n",
    "MODEL_PATH = \"/home/nculcwu/DeepSeek/deepseek-llm-7b-chat\"\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(f\"æº–å‚™å¾ '{MODEL_PATH}' è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨...\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"éŒ¯èª¤ï¼šæœªåµæ¸¬åˆ° CUDAã€‚æ­¤ç¨‹å¼ç¢¼éœ€è¦ NVIDIA GPU æ‰èƒ½åŸ·è¡Œã€‚\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"âœ… æ¨¡å‹å’Œåˆ†è©å™¨å·²æˆåŠŸè¼‰å…¥è‡³ GPUï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼éŒ¯èª¤è¨Šæ¯: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# æ­¥é©Ÿ 2: å®šç¾©åˆ†æ®µå‡½å¼\n",
    "# ----------------------------------------------------\n",
    "def segment_text(text: str):\n",
    "    \"\"\"å…ˆç”¨å¥è™Ÿåˆ†æ®µï¼Œå¯å†æ”¹æˆ herb-based åˆ†æ®µ\"\"\"\n",
    "    segments = re.split(r\"[ã€‚ï¼›]\", text)  # ä»¥ ã€‚ï¼› åˆ‡åˆ†\n",
    "    return [seg.strip() for seg in segments if seg.strip()]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# æ­¥é©Ÿ 3: å®šç¾©ç¿»è­¯å‡½å¼\n",
    "# ----------------------------------------------------\n",
    "def translate_to_plain_chinese(text: str) -> str:\n",
    "    \"\"\"å‘¼å«æ¨¡å‹é€²è¡Œç¹é«”ç™½è©±æ–‡ç¿»è­¯\"\"\"\n",
    "    prompt = f\"å°‡ä»¥ä¸‹æ–‡è¨€æ–‡ç¿»è­¯æˆç¹é«”ä¸­æ–‡ç™½è©±æ–‡ï¼Œåªç¿»è­¯å…§å®¹ï¼Œä¸è¦åšè§£é‡‹æˆ–åˆ†é»ï¼š\\n\\n{text}\\n\\nç¿»è­¯ï¼š\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"ç¿»è­¯ï¼š\" in result:\n",
    "        result = result.split(\"ç¿»è­¯ï¼š\", 1)[-1].strip()\n",
    "    return result\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# æ­¥é©Ÿ 4: æ‰¾åˆ°è³‡æ–™å¤¾è£¡çš„æ‰€æœ‰ txt æª”ï¼Œé€æª”ç¿»è­¯\n",
    "# ----------------------------------------------------\n",
    "input_dir = r\".\"\n",
    "output_dir = os.path.join(input_dir, \"translated\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".txt\") and file_name != \"ç¿»è­¯çµæœ.txt\":\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        print(f\"ğŸ“– æ­£åœ¨è™•ç†ï¼š{file_name}\")\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()\n",
    "        \n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        # åˆ†æ®µ\n",
    "        segments = segment_text(content)\n",
    "        translated_results = []\n",
    "\n",
    "        for seg in segments:\n",
    "            translated = translate_to_plain_chinese(seg)\n",
    "            translated_results.append(f\"ã€åŸæ–‡ã€‘\\n{seg}\\n\\nã€ç¿»è­¯ã€‘\\n{translated}\\n\")\n",
    "\n",
    "        # æ¯å·è¼¸å‡ºç¨ç«‹æª”æ¡ˆ\n",
    "        output_file = os.path.join(output_dir, file_name.replace(\".txt\", \"_translated.txt\"))\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\\n\".join(translated_results))\n",
    "\n",
    "        print(f\"âœ… ç¿»è­¯å®Œæˆï¼š{output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
