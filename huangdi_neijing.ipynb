{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 步驟 1: 載入模型\n",
    "# ----------------------------------------------------\n",
    "MODEL_PATH = \"/home/nculcwu/DeepSeek/deepseek-llm-7b-chat\"\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(f\"準備從 '{MODEL_PATH}' 載入模型和分詞器...\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"錯誤：未偵測到 CUDA。此程式碼需要 NVIDIA GPU 才能執行。\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"✅ 模型和分詞器已成功載入至 GPU！\")\n",
    "except Exception as e:\n",
    "    print(f\"模型載入失敗！錯誤訊息: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 步驟 2: 定義翻譯函式\n",
    "# ----------------------------------------------------\n",
    "def translate_to_plain_chinese(text: str) -> str:\n",
    "    \"\"\"呼叫模型進行繁體白話文翻譯\"\"\"\n",
    "    prompt = f\"將以下文言文翻譯成繁體中文白話文，只翻譯內容，不要做解釋或分點：\\n\\n{text}\\n\\n翻譯：\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # 從生成結果中抽取「翻譯」部分\n",
    "    if \"翻譯：\" in result:\n",
    "        result = result.split(\"翻譯：\", 1)[-1].strip()\n",
    "    return result\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 步驟 3: 找到資料夾裡的所有 txt 檔，逐檔翻譯\n",
    "# ----------------------------------------------------\n",
    "input_dir = r\".\"\n",
    "output_file = os.path.join(input_dir, \"翻譯結果.txt\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".txt\") and file_name != \"翻譯結果.txt\":\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        print(f\"📖 正在處理：{file_name}\")\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()\n",
    "        \n",
    "        if content:\n",
    "            translated = translate_to_plain_chinese(content)\n",
    "            all_results.append(f\"=== {file_name.replace('.txt','')} ===\\n{translated}\\n\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 步驟 4: 輸出到單一檔案\n",
    "# ----------------------------------------------------\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(all_results))\n",
    "\n",
    "print(f\"\\n✅ 所有翻譯已完成，結果輸出至：{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 步驟 1: 載入模型\n",
    "# ----------------------------------------------------\n",
    "MODEL_PATH = \"/home/nculcwu/DeepSeek/deepseek-llm-7b-chat\"\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(f\"準備從 '{MODEL_PATH}' 載入模型和分詞器...\")\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"錯誤：未偵測到 CUDA。此程式碼需要 NVIDIA GPU 才能執行。\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"✅ 模型和分詞器已成功載入至 GPU！\")\n",
    "except Exception as e:\n",
    "    print(f\"模型載入失敗！錯誤訊息: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 步驟 2: 定義分段函式\n",
    "# ----------------------------------------------------\n",
    "def segment_text(text: str):\n",
    "    \"\"\"先用句號分段，可再改成 herb-based 分段\"\"\"\n",
    "    segments = re.split(r\"[。；]\", text)  # 以 。； 切分\n",
    "    return [seg.strip() for seg in segments if seg.strip()]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 步驟 3: 定義翻譯函式\n",
    "# ----------------------------------------------------\n",
    "def translate_to_plain_chinese(text: str) -> str:\n",
    "    \"\"\"呼叫模型進行繁體白話文翻譯\"\"\"\n",
    "    prompt = f\"將以下文言文翻譯成繁體中文白話文，只翻譯內容，不要做解釋或分點：\\n\\n{text}\\n\\n翻譯：\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"翻譯：\" in result:\n",
    "        result = result.split(\"翻譯：\", 1)[-1].strip()\n",
    "    return result\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 步驟 4: 找到資料夾裡的所有 txt 檔，逐檔翻譯\n",
    "# ----------------------------------------------------\n",
    "input_dir = r\".\"\n",
    "output_dir = os.path.join(input_dir, \"translated\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".txt\") and file_name != \"翻譯結果.txt\":\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        print(f\"📖 正在處理：{file_name}\")\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()\n",
    "        \n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        # 分段\n",
    "        segments = segment_text(content)\n",
    "        translated_results = []\n",
    "\n",
    "        for seg in segments:\n",
    "            translated = translate_to_plain_chinese(seg)\n",
    "            translated_results.append(f\"【原文】\\n{seg}\\n\\n【翻譯】\\n{translated}\\n\")\n",
    "\n",
    "        # 每卷輸出獨立檔案\n",
    "        output_file = os.path.join(output_dir, file_name.replace(\".txt\", \"_translated.txt\"))\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\\n\".join(translated_results))\n",
    "\n",
    "        print(f\"✅ 翻譯完成：{output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
